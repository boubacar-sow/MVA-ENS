{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsD-LMKT7XMt"
      },
      "source": [
        "\n",
        "<center><h2>ALTeGraD 2023<br>Lab Session 4: NLP Frameworks</h2> 07 / 11 / 2023<br> Dr. G. Shang, H. Abdine<br><br>\n",
        "\n",
        "\n",
        "<b>Student name:</b> Boubacar Sow\n",
        "\n",
        "</center>\n",
        "\n",
        "In this lab you will learn how to use Fairseq and HuggingFace transformers - The most used libraries by researchers and developers  and finetune language models - to finetune a pretrained French language model ($RoBERTa_{small}^{fr}$) on the sentiment analysis dataset CLS_Books where each review is labeled as positive or negative and finetune a variant of BLOOM on a question/answer dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwN3KCm5Ec6r"
      },
      "source": [
        "## <b>Preparing the environment and installing libraries, models and data for Part 1 and Part 2</b>\n",
        "\n",
        "In this section, we will setup the environment on Google Colab (first cell), download the pretraind model (second cell) and the finetuning dataset (third cell). In case you are using your personal computer maket sure to:\n",
        "\n",
        "1- Use Ubuntu (or any similar linux distribution) or MacOS. <b> P.S. In case you have Windows, please use Google Colab. </b>\n",
        "\n",
        "2- <b>Use Anaconda</b> and create new environment if you already installed Fairseq since we will be using a slightly modified version of this library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cGqZ9ZB84Cd",
        "outputId": "9e854575-c9f8-4a88-fe1a-4c57cfb8b1bc"
      },
      "outputs": [],
      "source": [
        "# !mkdir altegrad.lab3 && cd altegrad.lab3 && mkdir libs\n",
        "# %cd altegrad.lab3/libs\n",
        "# !git clone https://github.com/hadi-abdine/fairseq\n",
        "# !pip install git+https://github.com/hadi-abdine/fairseq\n",
        "# !git clone https://github.com/huggingface/transformers.git\n",
        "# !pip install git+https://github.com/huggingface/transformers.git\n",
        "# !pip install datasets\n",
        "# !pip install evaluate\n",
        "# !pip install sentencepiece\n",
        "#!pip install tensorboardX\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !git clone https://github.com/hadi-abdine/fairseq\n",
        "# !cd fairseq\n",
        "# !pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rxHffsPm-EqW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/boubacar/Documents/ALTEGRAD/TP4/altegrad.lab3/models\n",
            "--2023-11-09 09:03:31--  https://nuage.lix.polytechnique.fr/index.php/s/E4otcD7B9jm2AWx/download/RoBERTa_small_fr.zip\n",
            "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
            "Resolving nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)... 193.55.176.11\n",
            "Connecting to nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)|193.55.176.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 264887103 (253M) [application/zip]\n",
            "Saving to: ‘model_fairseq.zip’\n",
            "\n",
            "model_fairseq.zip   100%[===================>] 252.62M  4.45MB/s    in 61s     \n",
            "\n",
            "2023-11-09 09:04:33 (4.13 MB/s) - ‘model_fairseq.zip’ saved [264887103/264887103]\n",
            "\n",
            "Archive:  model_fairseq.zip\n",
            "   creating: RoBERTa_small_fr/\n",
            "  inflating: RoBERTa_small_fr/model.pt  \n",
            "  inflating: __MACOSX/RoBERTa_small_fr/._model.pt  \n",
            "  inflating: RoBERTa_small_fr/sentencepiece.bpe.model  \n",
            "  inflating: RoBERTa_small_fr/dict.txt  \n",
            "--2023-11-09 09:04:39--  https://nuage.lix.polytechnique.fr/index.php/s/yYQjg9XWekttG5j/download/RoBERTa_small_fr_HuggingFace.zip\n",
            "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
            "Resolving nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)... 193.55.176.11\n",
            "Connecting to nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)|193.55.176.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53984187 (51M) [application/zip]\n",
            "Saving to: ‘model_huggingface.zip’\n",
            "\n",
            "model_huggingface.z 100%[===================>]  51.48M  4.25MB/s    in 12s     \n",
            "\n",
            "2023-11-09 09:04:52 (4.19 MB/s) - ‘model_huggingface.zip’ saved [53984187/53984187]\n",
            "\n",
            "Archive:  model_huggingface.zip\n",
            "   creating: RoBERTa_small_fr_HuggingFace/\n",
            "  inflating: RoBERTa_small_fr_HuggingFace/sentencepiece.bpe.model  \n",
            "  inflating: RoBERTa_small_fr_HuggingFace/config.json  \n",
            "  inflating: RoBERTa_small_fr_HuggingFace/pytorch_model.bin  \n"
          ]
        }
      ],
      "source": [
        "# !cd .. && mkdir models\n",
        "# %cd ../models\n",
        "# \n",
        "# !wget -c 'https://nuage.lix.polytechnique.fr/index.php/s/E4otcD7B9jm2AWx/download/RoBERTa_small_fr.zip' -O \"model_fairseq.zip\"\n",
        "# !unzip model_fairseq.zip\n",
        "# !rm model_fairseq.zip\n",
        "# !rm -rf __MACOSX/\n",
        "# \n",
        "# !wget -c \"https://nuage.lix.polytechnique.fr/index.php/s/yYQjg9XWekttG5j/download/RoBERTa_small_fr_HuggingFace.zip\" -O \"model_huggingface.zip\"\n",
        "# !unzip model_huggingface.zip\n",
        "# !rm model_huggingface.zip\n",
        "# !rm -rf __MACOSX/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HfZ_znATNdya"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/boubacar/Documents/ALTEGRAD/TP4/altegrad.lab3/data\n",
            "--2023-11-09 09:06:07--  https://nuage.lix.polytechnique.fr/index.php/s/EBHqfR776oCE2Nj/download/cls.books.zip\n",
            "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
            "Resolving nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)... 193.55.176.11\n",
            "Connecting to nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)|193.55.176.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 955190 (933K) [application/zip]\n",
            "Saving to: ‘cls.books.zip’\n",
            "\n",
            "cls.books.zip       100%[===================>] 932.80K  3.83MB/s    in 0.2s    \n",
            "\n",
            "2023-11-09 09:06:07 (3.83 MB/s) - ‘cls.books.zip’ saved [955190/955190]\n",
            "\n",
            "Archive:  cls.books.zip\n",
            "   creating: cls.books/\n",
            "  inflating: cls.books/valid.label   \n",
            "  inflating: cls.books/test.review   \n",
            "  inflating: cls.books/valid.review  \n",
            "  inflating: cls.books/train.review  \n",
            "  inflating: cls.books/train.label   \n",
            "  inflating: cls.books/test.label    \n",
            "/home/boubacar/Documents/ALTEGRAD/TP4/altegrad.lab3\n"
          ]
        }
      ],
      "source": [
        "# !cd .. && mkdir data\n",
        "# %cd ../data\n",
        "# !wget -c \"https://nuage.lix.polytechnique.fr/index.php/s/EBHqfR776oCE2Nj/download/cls.books.zip\" -O \"cls.books.zip\"\n",
        "# !unzip cls.books.zip\n",
        "# !rm cls.books.zip\n",
        "# !rm -rf __MACOSX/\n",
        "# !mkdir cls.books-json\n",
        "# %cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/boubacar/Documents/ALTEGRAD/TP4/altegrad.lab3\n"
          ]
        }
      ],
      "source": [
        "%cd altegrad.lab3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H40TxVIvEWyu"
      },
      "source": [
        "# <b>Part 1: Fairseq</b>\n",
        "\n",
        "In the first part of this lab, you will finetune the given model on model on CLS_Books dataset using <b>Fairseq</b> by following these steps:<br>\n",
        "\n",
        " 1- <b>Tokenize the reviews</b> (Train, Valid and Test) using trained sentencepiece tokenizer provided alongside the pretrained model.[using sentencepiece library and setting the parameter <b>out_type=str</b> in the encode function].<br>\n",
        " 2- <b>Binarize the tokenized reviews and their labels</b> using the preprocess python script provided in Fairseq.<br>\n",
        " 3- <b>Fintune the pretrained $RoBERTa_{small}^{fr}$ model</b> using the train python script provided in Fairseq.<br>\n",
        "\n",
        " Finally, you will finish the first part by training a random $RoBERTa_{small}^{fr}$ model on the CLS_Books dataset and compare the results against the pretrained model while <b>visualizing the accuracies on tensorboard</b>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvpyZEexOXHm"
      },
      "source": [
        "## <b> Number of parameters of the model</b>\n",
        "\n",
        "In this section you have to compute the number of parameters of $RoBERTa_{small}^{fr}$ using PyTorch (Only the base model with out the head). (<b>Hint:</b> you can check the architecture of the model using model['model'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "j7isz60LOwlV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The total number of parameters:  22807552\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "model = torch.load(\"models/RoBERTa_small_fr/model.pt\")\n",
        "\n",
        "n_parameters = 0\n",
        "for layer_name, parameter in model[\"model\"].items():\n",
        "    if \"weight\" in layer_name and not \"norm\" in layer_name and not \"lm_head\" in layer_name:\n",
        "        n_parameters += parameter.numel()\n",
        "\n",
        "print(\"The total number of parameters: \", n_parameters)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz8fnWOSI0eF"
      },
      "source": [
        "## <b>Tokenizing the reviews</b>\n",
        "\n",
        "In this section we will tokenize the finetuning dataset using sentenpiece tokenizer. We have three splits in our datase: train valid and test sets.\n",
        "\n",
        "In this task you have to use the trained sentencepiece tokenizer (RoBERTa_small_fr/sentencepiece.bpe.model) to tokenize the three files <b>train.review</b>, <b>valid.review</b> and <b>test.review</b> and output the three files <b>train.spm.review</b>, <b>valid.spm.review</b> and <b>test.spm.review</b> containing the tokenized reviews.\n",
        "\n",
        "Documentation: https://github.com/google/sentencepiece#readme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "D-hOlotmOW7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▁Ce ▁livre ▁explique ▁technique ment ▁et ▁de ▁façon ▁très ▁com pré hen sible , ▁même ▁pour ▁des ▁né o phy tes ▁en ▁physique ▁des ▁matériaux , ▁comment ▁et ▁pourquoi ▁la ▁version ▁officielle ▁concernant ▁l ' ef fond re ment ▁des ▁3 ▁tour s ▁du ▁W TC ▁ne ▁tient ▁tout ▁simplement ▁pas ▁de bou t . ▁Et ▁c ' est ▁sans ▁appel ▁! ▁Il ▁constitu e ▁un ▁très ▁bon ▁argument aire ▁scientifique ▁permettant ▁de ▁soutenir ▁l ' hy po th èse ▁la ▁plus ▁vrai sem bla ble ▁pour ▁explique r ▁ce ▁qui ▁s ' est ▁réellement ▁passé ▁à ▁New ▁York , ▁ce ▁jour - là , ▁l ' hy po th èse ▁d ' une ▁dé moli tion ▁cont rô lé e ▁! ▁A ▁lire ▁absolument ▁!\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "s = spm.SentencePieceProcessor(model_file='models/RoBERTa_small_fr/sentencepiece.bpe.model')\n",
        "\n",
        "SPLITS=['train', 'test', 'valid']\n",
        "SENTS=\"review\"\n",
        "\n",
        "for split in SPLITS:\n",
        "    with open('data/cls.books/'+split+'.'+SENTS, 'r') as f:\n",
        "        reviews = f.readlines()\n",
        "        # tokenize the data using s.encode and a loop(check the documentation)\n",
        "        reviews =  [' '.join(s.encode(review, out_type=str)) for review in reviews]\n",
        "\n",
        "        # It should look something like this :\n",
        "        #▁An ci enne ▁VS ▁Nouvelle ▁version ▁plus\n",
        "\n",
        "    with open('data/cls.books/'+split+'.spm.'+SENTS, 'w') as f:\n",
        "        for review in reviews:\n",
        "          f.write(review+'\\n')\n",
        "print(reviews[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGNK19XuKBk0"
      },
      "source": [
        "## <b>Binarizing/Preprocessing the finetuning dataset</b>\n",
        "\n",
        "In this section, you have to binarize the CLS_Books dataset using the <b>fairseq/fairseq_cli/preprocess.py</b> script:\n",
        "\n",
        "1- Binarize the tokenized reviews and put the output in <b>data/cls-books-bin/input0</b>. Note: Our pretrained model's embedding matrix contains only the embedding of the vocab listed in the dictionary <b>dict.txt</b>. You need to use the dictionary in the binarization of the text to transform the tokens into indices. Also note that we are using Encoder only architecture, so we only have source data.\n",
        "\n",
        "2- Binarize the labels (train.label, valid.label and test.label files) and put the output in <b>data/cls-books-bin/label</b>.\n",
        "\n",
        "Documentation: https://fairseq.readthedocs.io/en/latest/command_line_tools.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "jIF1wvWoFp4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 2: syntax error: unexpected end of file\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/boubacar/Documents/ALTEGRAD/TP4/altegrad.lab3/libs/fairseq/fairseq_cli/preprocess.py\", line 18, in <module>\n",
            "    from fairseq import options, tasks, utils\n",
            "ModuleNotFoundError: No module named 'fairseq'\n"
          ]
        }
      ],
      "source": [
        "!(python libs/fairseq/fairseq_cli/preprocess.py \\\n",
        "    --srcdict models/RoBERTa_small_fr/dict.txt \\\n",
        "    --only-source \\\n",
        "    --trainpref data/cls.books/train.spm.review \\\n",
        "    --validpref data/cls.books/valid.spm.review \\\n",
        "    --testpref data/cls.books/test.spm.review \\\n",
        "    --destdir data/cls.books-bin\\input0 \\\n",
        "    --workers 8 \\)#fill me - binarize the tokenized reviews\n",
        "\n",
        "# Binarize the labels \n",
        "!(python libs/fairseq/fairseq_cli/preprocess.py \\\n",
        "    --only-source\\\n",
        "    --trainpref data/cls.books/train.label\\\n",
        "    --validpref data/cls.books/valid.label\\\n",
        "    --testpref data/cls.books/test.label\\\n",
        "    --destdir data/cls.books-bin/label\\\n",
        "    --workers 8)#fill me - binarize the labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SSjBcQJnuSL"
      },
      "source": [
        "## <b>Finetuning $RoBERTa_{small}^{fr}$</b>\n",
        "\n",
        "In this section you will use <b>fairseq/fairseq_cli/train.py</b> python script to finetune the pretrained model on the CLS_Books dataset (binarized data) for three different seeds: 0, 1 and 2.\n",
        "\n",
        "Make sure to use the following hyper-parameters: $\\textit{batch size}=8, \\textit{max number of epochs}: 5, \\textit{optimizer}: Adam, \\textit{max learning rate}: 1e-05,  \\textit{warm up ratio}: 0.06, \\textit{learning rate scheduler}: linear$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "JV2112YPJEDA"
      },
      "outputs": [],
      "source": [
        "DATA_SET='books'\n",
        "TASK= \"sentence_prediction\"# fill me, sentence prediction task on fairseq\n",
        "MODEL='RoBERTa_small_fr'\n",
        "DATA_PATH= \"data/cls.books-bin\"\n",
        "MODEL_PATH= \"models/RoBERTa_small_fr/model.pt\"\n",
        "MAX_EPOCH= 5\n",
        "MAX_SENTENCES= 8\n",
        "MAX_UPDATE= int(MAX_EPOCH * 1800 / MAX_SENTENCES)\n",
        "LR= 1e-05\n",
        "VALID_SUBSET='valid,test' # for simplicity we will validate on both valid and test set, and then pick the value of test set corresponding the best validation score.\n",
        "METRIC = \"accuracy\"\n",
        "NUM_CLASSES=2\n",
        "SEEDS=3\n",
        "CUDA_VISIBLE_DEVICES=0\n",
        "WARMUP = int(0.06 * MAX_UPDATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "6Mdznms-EYyz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/boubacar/Documents/ALTEGRAD/TP4/altegrad.lab3/libs/fairseq/fairseq_cli/train.py\", line 28, in <module>\n",
            "    from omegaconf import DictConfig, OmegaConf\n",
            "ModuleNotFoundError: No module named 'omegaconf'\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/boubacar/Documents/ALTEGRAD/TP4/altegrad.lab3/libs/fairseq/fairseq_cli/train.py\", line 28, in <module>\n",
            "    from omegaconf import DictConfig, OmegaConf\n",
            "ModuleNotFoundError: No module named 'omegaconf'\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/boubacar/Documents/ALTEGRAD/TP4/altegrad.lab3/libs/fairseq/fairseq_cli/train.py\", line 28, in <module>\n",
            "    from omegaconf import DictConfig, OmegaConf\n",
            "ModuleNotFoundError: No module named 'omegaconf'\n"
          ]
        }
      ],
      "source": [
        "for SEED in range(SEEDS):\n",
        "  TENSORBOARD_LOGS= 'tensorboard_logs/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  !(python libs/fairseq/fairseq_cli/train.py $DATA_PATH \\\n",
        "                --restore-file $MODEL_PATH \\\n",
        "                --batch-size $MAX_SENTENCES \\\n",
        "                --task $TASK \\\n",
        "                --update-freq 1 \\\n",
        "                --seed $SEED \\\n",
        "                --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "                --init-token 0 \\\n",
        "                --separator-token 2 \\\n",
        "                --arch roberta_small \\\n",
        "                --criterion sentence_prediction \\\n",
        "                --num-classes $NUM_CLASSES \\\n",
        "                --weight-decay 0.01 \\\n",
        "                --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-08 \\\n",
        "                --maximize-best-checkpoint-metric \\\n",
        "                --best-checkpoint-metric 'accuracy' \\\n",
        "                --save-dir $SAVE_DIR \\\n",
        "                --lr-scheduler polynomial_decay \\\n",
        "                --lr $LR \\\n",
        "                --max-update $MAX_UPDATE \\\n",
        "                --total-num-update $MAX_UPDATE \\\n",
        "                --no-epoch-checkpoints \\\n",
        "                --no-last-checkpoints \\\n",
        "                --tensorboard-logdir $TENSORBOARD_LOGS \\\n",
        "                --log-interval 5 \\\n",
        "                --warmup-updates $WARMUP \\\n",
        "                --max-epoch $MAX_EPOCH \\\n",
        "                --keep-best-checkpoints 1 \\\n",
        "                --max-positions 256 \\\n",
        "                --valid-subset $VALID_SUBSET \\\n",
        "                --shorten-method 'truncate' \\\n",
        "                --no-save \\\n",
        "                --distributed-world-size 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi1U19Uunnse"
      },
      "source": [
        "## <b>Random $RoBERTa_{small}^{fr}$ model training:</b>\n",
        "\n",
        "In this section you have to finetune a random checkpinf of the model $RoBERTa_{small}^{fr}$ using the same setting as before (<b>Hint:</b> an unexisted model path will not give you an error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSLWP6VUhUlC"
      },
      "outputs": [],
      "source": [
        "DATA_SET='books'\n",
        "TASK= \"sentence_prediction\"# fill me, sentence prediction task on fairseq\n",
        "MODEL='RoBERTa_small_fr'\n",
        "DATA_PATH= \"data/cls.books-bin\"\n",
        "MODEL_PATH= \"models/RoBERTa_small_fr/model.pt\"\n",
        "MAX_EPOCH= 5\n",
        "MAX_SENTENCES= 8\n",
        "MAX_UPDATE= int(MAX_EPOCH * 1800 / MAX_SENTENCES)\n",
        "LR= 1e-05\n",
        "VALID_SUBSET='valid,test' # for simplicity we will validate on both valid and test set, and then pick the value of test set corresponding the best validation score.\n",
        "METRIC = \"accuracy\"\n",
        "NUM_CLASSES=2\n",
        "SEEDS=3\n",
        "CUDA_VISIBLE_DEVICES=0\n",
        "WARMUP = int(0.06 * MAX_UPDATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8067628972661017"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model1_seed1_test = [67.7, 73.9, 79.3, 78.9, 79.9]\n",
        "model1_seed2_test = [72, 76.4, 78.5, 78.7, 79.5]\n",
        "model1_seed3_test = [71.5, 75.9, 79.6, 78.41, 80.4]\n",
        "var = np.std([model1_seed1_test, model1_seed2_test, model1_seed3_test], axis=0)\n",
        "np.mean(var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtsCysc4hb42"
      },
      "outputs": [],
      "source": [
        "for SEED in range(SEEDS):\n",
        "  TENSORBOARD_LOGS= 'tensorboard_logs/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  !(python libs/fairseq/fairseq_cli/train.py $DATA_PATH \\\n",
        "                --restore-file $MODEL_PATH \\\n",
        "                --batch-size $MAX_SENTENCES \\\n",
        "                --task $TASK \\\n",
        "                --update-freq 1 \\\n",
        "                --seed $SEED \\\n",
        "                --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "                --init-token 0 \\\n",
        "                --separator-token 2 \\\n",
        "                --arch roberta_small \\\n",
        "                --criterion sentence_prediction \\\n",
        "                --num-classes $NUM_CLASSES \\\n",
        "                --weight-decay 0.01 \\\n",
        "                --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-08 \\\n",
        "                --maximize-best-checkpoint-metric \\\n",
        "                --best-checkpoint-metric 'accuracy' \\\n",
        "                --save-dir $SAVE_DIR \\\n",
        "                --lr-scheduler polynomial_decay \\\n",
        "                --lr $LR \\\n",
        "                --max-update $MAX_UPDATE \\\n",
        "                --total-num-update $MAX_UPDATE \\\n",
        "                --no-epoch-checkpoints \\\n",
        "                --no-last-checkpoints \\\n",
        "                --tensorboard-logdir $TENSORBOARD_LOGS \\\n",
        "                --log-interval 5 \\\n",
        "                --warmup-updates $((6*$MAX_UPDATE/100)) \\\n",
        "                --max-epoch $MAX_EPOCH \\\n",
        "                --keep-best-checkpoints 1 \\\n",
        "                --max-positions 256 \\\n",
        "                --valid-subset $VALID_SUBSET \\\n",
        "                --shorten-method 'truncate' \\\n",
        "                --no-save \\\n",
        "                --distributed-world-size 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHACXaPSLwu4"
      },
      "source": [
        "## <b>Tensorboard Visualisation </B>\n",
        "\n",
        "In the this we will use tensorboard to visualize the training, validation and test accuracies. <b>Include and analyse in you report a screenshot of the test accuracy of the six models</b>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwVvJNExS2dl"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir tensorboard_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAR_P343MCKC"
      },
      "source": [
        "# <b>Part 2: Finetuning $RoBERTa_{small}^{fr}$ using HuggingFace's Transfromers</b>\n",
        "In this section of the lab, we will finetune a HuggingFace checkpoint of our $RoBERTa_{small}^{fr}$ on the CLS_Books dataset. Like in the first part we will start by downloading the HuggingFace checkpoint and <b>preparing a json format of the CLS_Books dataset</b> (Which is suitable for HuggingFace's checkpoints finetuning using their run_glue script)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3M090L45oPn"
      },
      "source": [
        "## <b>Converting the CLS_Books dataset to json line files</b>\n",
        "\n",
        "Unlike Fairseq, you do not need to perform tokenization and binarization in Hugging Face transformer library. However, in order to use the implemented script in the transformers library, you need to convert your data to json line files (for each split: train, valid and test)\n",
        "\n",
        "for instance, each line inside you file will consist of one and one sample only, contaning the review (accessed by the key <i>sentence1</i> and its label, accessed by the key <i>label</i>. Below you can find an example from <i>valid.json</i> file.\n",
        "\n",
        "Note that these instructions are not valid for all kind of tasks. For other types of tasks (supported in Hugging face) you have to refer to their github for more details.<br>\n",
        "\n",
        "---------------------------------------------------------------------\n",
        "<i>\n",
        "{\"sentence1\":\"Seul ouvrage fran\\u00e7ais sur le th\\u00e8me Produits Structur\\u00e9s \\/ fonds \\u00e0 formule, il permet de fa\\u00e7on p\\u00e9dagogique d'appr\\u00e9hender parfaitement les m\\u00e9canismes financiers utilis\\u00e9s. Une r\\u00e9f\\u00e9rence pour ceux qui veulent comprendre les technicit\\u00e9s de base et les raisons de l'engouement des investisseurs sur ces actifs \\u00e0 hauteur de plusieurs milliards d'euros.\",\"label\":\"1\"}<br>\n",
        "{\"sentence1\":\"Livre tr\\u00e8s int\\u00e9ressant !  mais si comme moi vous cherchez des \\\"infos\\\" sur les techniques de sorties et autres \\\"modes d'emploi\\\", afin de vivre par vous m\\u00eame ce genre d'exp\\u00e9rience, c'est pas le bon livre.  \\u00e7a ne lui enl\\u00e8ve d'ailleurd rien \\u00e0 son int\\u00earet.\",\"label\":\"0\"}\n",
        "</i>\n",
        "\n",
        "---------------------------------------------------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZZFHEHFyv5F"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "SPLITS=['train', 'test', 'valid']\n",
        "\n",
        "for split in SPLITS:\n",
        "    with open('data/cls.books/'+split+'.review', 'r') as f:\n",
        "        reviews = f.readlines()\n",
        "    with open('data/cls.books/'+split+'.label', 'r') as f:\n",
        "        labels = f.readlines()\n",
        "    with open('data/cls.books-json/'+split+'.json', 'w') as f:\n",
        "        #fill the gap here to create train.json, valid.json and test.json\n",
        "        data = pd.Dataframe([reviews, labels], index=[\"sentence\", \"label\"]).T\n",
        "        data[\"sentence\"] = data[\"sentence\"].apply(lambda x: x[:-1])\n",
        "        data[\"label\"] = data[\"label\"].apply(lambda x: x[:-1])\n",
        "        data.to_json(f, orient=\"records\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICnN2FvnhTbs"
      },
      "source": [
        "## <b>Finetuning $RoBERTa_{small}^{fr}$ using the Transformers Library</b>\n",
        "\n",
        "In order to finrtune the model using HuggingFace, you to use the <b>run_glue.py</b> Python script located in the transformers library. For more details, refer to <a href=\"https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification\" target=\"_blank\">the Huggingface/transformers repository on Github</a>. Make sure to use the same hyperparameter as in the first part of this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "0c6vM239VsAT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[K     |████████████████████████████████| 261 kB 3.2 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.17 in /home/boubacar/.local/lib/python3.11/site-packages (from accelerate) (1.24.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /home/boubacar/.local/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /home/boubacar/.local/lib/python3.11/site-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied, skipping upgrade: psutil in /home/boubacar/.local/lib/python3.11/site-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied, skipping upgrade: huggingface-hub in /home/boubacar/.local/lib/python3.11/site-packages (from accelerate) (0.19.0)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.10.0 in /home/boubacar/.local/lib/python3.11/site-packages (from accelerate) (2.1.0+cpu)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.42.1 in /home/boubacar/.local/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.3 in /usr/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.8.0)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /home/boubacar/.local/lib/python3.11/site-packages (from huggingface-hub->accelerate) (3.13.1)\n",
            "Requirement already satisfied, skipping upgrade: fsspec>=2023.5.0 in /home/boubacar/.local/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2023.10.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /home/boubacar/.local/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.30.0)\n",
            "Requirement already satisfied, skipping upgrade: sympy in /home/boubacar/.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /home/boubacar/.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied, skipping upgrade: networkx in /home/boubacar/.local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.8.8)\n",
            "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in /usr/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<3,>=1.21.1 in /home/boubacar/.local/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/boubacar/.local/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2023.5.7)\n",
            "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in /home/boubacar/.local/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: mpmath>=0.19 in /home/boubacar/.local/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=2.0 in /home/boubacar/.local/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.24.1\n",
            "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 23.3.1 is available.\n",
            "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "h-BBIykNjH7A"
      },
      "outputs": [],
      "source": [
        "DATA_SET='books'\n",
        "MODEL='RoBERTa_small_fr_huggingface'\n",
        "MAX_SENTENCES= 8# fill me, batch size.\n",
        "LR=1e-05 #fill me, learning rate\n",
        "MAX_EPOCH= 5 #fill me\n",
        "NUM_CLASSES= 2 #fill me\n",
        "SEEDS=3\n",
        "CUDA_VISIBLE_DEVICES=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV2Zla33hK_C"
      },
      "outputs": [],
      "source": [
        "for SEED in range(SEEDS):\n",
        "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  !(python libs/transformers/examples/pytorch/text-classification/run_glue.py \\\n",
        "    --model_name_or_path 'models/RoBERTa_small_fr_HuggingFace' \\\n",
        "    --task_name 'mrpc' \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --max_seq_length 256 \\\n",
        "    --per_device_train_batch_size $MAX_SENTENCES \\\n",
        "    --learning_rate $LR \\\n",
        "    --num_train_epochs $MAX_EPOCH \\\n",
        "    --output_dir $SAVE_DIR \\\n",
        "    --overwrite_output_dir \\\n",
        "    --save_total_limit 1 \\\n",
        "    --save_steps 10000 \\\n",
        "    --seed $SEED \\\n",
        "    --train_file 'data/cls.books-json/train.json' \\\n",
        "    --validation_file 'data/cls.books-json/valid.json' \\\n",
        "    --test_file 'data/cls.books-json/test.json' \\\n",
        "    --metric_for_best_model 'accuracy' \\\n",
        "    --load_best_model_at_end \\\n",
        "    --evaluation_strategy 'steps' \\\n",
        "    --eval_steps 10000 \\\n",
        "    --logging_steps 10000 \\\n",
        "    --save_steps 10000 \\\n",
        "    --logging_strategy epoch \\\n",
        "  )\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2UMHjatpFvm"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir checkpoints --port 6007"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUCV4V0ONKeJ"
      },
      "source": [
        "# <b>Part 3: Finetuning $BLOOM-560m$ using HuggingFace's Transfromers</b>\n",
        "In this part, we will fintune $BLOOM-560m$:https://huggingface.co/bigscience/bloom-560m on a question/answer dataset. We will equally use LoRA and quantization during the finetuning.\n",
        "\n",
        "## <b>Preparing the environment and installing libraries:<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-VOzEaS7Wpv"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq bitsandbytes==0.39.0 --progress-bar off\n",
        "!pip install -qqq torch==2.0.1 --progress-bar off\n",
        "!pip install -qqq -U git+https://github.com/huggingface/transformers.git@e03a9cc --progress-bar off\n",
        "!pip install -qqq -U git+https://github.com/huggingface/peft.git@42a184f --progress-bar off\n",
        "!pip install -qqq -U git+https://github.com/huggingface/accelerate.git@c9fbb71 --progress-bar off\n",
        "!pip install -qqq datasets==2.12.0 --progress-bar off\n",
        "!pip install -qqq loralib==0.1.1 --progress-bar off\n",
        "!pip install -qqq einops==0.6.1 --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qHHXf0xHUsx9"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'bitsandbytes'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb Cell 33\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpprint\u001b[39;00m \u001b[39mimport\u001b[39;00m pprint\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X44sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mbnb\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X44sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X44sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bitsandbytes'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from pprint import pprint\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftConfig,\n",
        "    PeftModel,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC-Kv8g8MSuW"
      },
      "source": [
        "## <b>Loading the model and the tokenizer:</b>\n",
        "In this section, we will load the BLOOM model while using the BitsAndBytes library for quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X6TaXDnRVKDq"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'BitsAndBytesConfig' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb Cell 35\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m MODEL_NAME \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbigscience/bloom-560m\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m bnb_config \u001b[39m=\u001b[39m BitsAndBytesConfig\u001b[39m.\u001b[39mfrom_pretrained(MODEL_NAME)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     MODEL_NAME,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X46sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     device_map\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X46sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X46sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     quantization_config\u001b[39m=\u001b[39mbnb_config,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X46sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X46sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(MODEL_NAME)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BitsAndBytesConfig' is not defined"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = \"bigscience/bloom-560m\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig.from_pretrained(MODEL_NAME)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LIvuxW4lVW_E"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        # fill the gap: get the number of trainable parameters: trainable_params\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTgKyxhJMeEP"
      },
      "source": [
        "## <b>Configuring LoRA:<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "duTYSKKYVamH"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'LoraConfig' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb Cell 38\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m config \u001b[39m=\u001b[39m LoraConfig(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     r\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     lora_alpha\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     target_modules\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mquery_key_value\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     lora_dropout\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X52sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     bias\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X52sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     task_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCAUSAL_LM\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X52sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X52sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model \u001b[39m=\u001b[39m get_peft_model(model, config)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X52sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m print_trainable_parameters(model)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LoraConfig' is not defined"
          ]
        }
      ],
      "source": [
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H1bBQaSNVsr"
      },
      "source": [
        "## <b>Test the model before finetuning:<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sRW7HPX6WCmI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<human>: Comment je peux créer un compte?  \n",
            " <assistant>: \n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'generation_config'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb Cell 40\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<human>: Comment je peux créer un compte?  \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m <assistant>: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(prompt)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m generation_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgeneration_config\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X54sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m generation_config\u001b[39m.\u001b[39mmax_new_tokens \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m generation_config\u001b[39m.\u001b[39mtemperature \u001b[39m=\u001b[39m \u001b[39m0.7\u001b[39m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'generation_config'"
          ]
        }
      ],
      "source": [
        "# fill the gap, prompt of the format: \"<human>: Comment je peux créer un compte?  \\n <assistant>: \", with an empty response from the assistant\n",
        "prompt = \"<human>: Comment je peux créer un compte?  \\n <assistant>: \"\n",
        "print(prompt)\n",
        "\n",
        "\n",
        "generation_config = model.generation_config\n",
        "generation_config.max_new_tokens = 200\n",
        "generation_config.temperature = 0.7\n",
        "generation_config.top_p = 0.7\n",
        "generation_config.num_return_sequences = 1\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DnmKXlqSWPQq"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m<timed exec>:3\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "device = \"cuda:0\"\n",
        "\n",
        "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoding.input_ids,\n",
        "        attention_mask=encoding.attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbhQySqVMo2T"
      },
      "source": [
        "## <b>Loading the question/answer dataset from HuggingFace:<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zR54r9AWQ-d"
      },
      "outputs": [],
      "source": [
        "data = load_dataset(\"OpenLLM-France/Tutoriel\", data_files=\"ecommerce-faq-fr.json\")\n",
        "pd.DataFrame(data[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb Cell 44\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/boubacar/Documents/ALTEGRAD/TP4/Copy_of_Altegrad_2023_lab4_handout.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oSZX9UcNBsu"
      },
      "source": [
        "## <b>Preparing the finetuning data:<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQiJpF41WZEc"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(data_point):\n",
        "    # fill the gap, transform the data into prompts of the format: \"<human>: question?  \\n <assistant>: response\"\n",
        "    return \"<human>: \" + data_point[\"question\"] + \" \\n <assistant>: \" + data_point[\"answer\"]\n",
        "\n",
        "\n",
        "def generate_and_tokenize_prompt(data_point):\n",
        "    full_prompt = generate_prompt(data_point)\n",
        "    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
        "    return tokenized_full_prompt\n",
        "\n",
        "data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGfbqJ_cNHDa"
      },
      "source": [
        "## <b>Finetuning:<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjBMVb6yW_74"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIR = \"experiments\"\n",
        "\n",
        "training_args = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    save_total_limit=3,\n",
        "    logging_steps=1,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    max_steps=80,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.05,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data,\n",
        "    args=training_args,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B01QbSicXknK"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir experiments/runs --port 6008"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxy9b1f4Nqpd"
      },
      "source": [
        "## <b>Test the model after the finetuning:<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCYynNlrXDhf"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "device = \"cuda:0\"\n",
        "\n",
        "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoding.input_ids,\n",
        "        attention_mask=encoding.attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS_lwrJdXr-Y"
      },
      "outputs": [],
      "source": [
        "def generate_response(question: str) -> str:\n",
        "    # fill the gap, transform the data into prompts of the format: \"<human>: question?  \\n <assistant>: \" with an empty response\n",
        "    prompt = \"<human>: \" + question + \" \\n <assistant>: \"\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(\n",
        "            input_ids=encoding.input_ids,\n",
        "            attention_mask=encoding.attention_mask,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    assistant_start = \"<assistant>:\"\n",
        "    response_start = response.find(assistant_start)\n",
        "    return response[response_start + len(assistant_start) :].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYaO6H_hXsvG"
      },
      "outputs": [],
      "source": [
        "prompt = \"Puis-je retourner un produit s'il s'agit d'un article en liquidation ou en vente finale ?\"\n",
        "print('-', prompt,'\\n')\n",
        "print(generate_response(prompt))\n",
        "\n",
        "prompt = \"Que se passe-t-il lorsque je retourne un article en déstockage ?\"\n",
        "print('\\n\\n\\n-', prompt, '\\n')\n",
        "print(generate_response(prompt))\n",
        "\n",
        "print('\\n\\n\\n-', prompt, '\\n')\n",
        "prompt = \"Comment puis-je savoir quand je recevrai ma commande ?\"\n",
        "print(generate_response(prompt))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
